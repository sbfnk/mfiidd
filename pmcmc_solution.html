<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Full example: particle MCMC</title>

<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-6.4.0/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.0/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">MFIIDD 2023</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="introduction.html">Introduction</a>
</li>
<li>
  <a href="mcmc.html">MCMC</a>
</li>
<li>
  <a href="mcmc_diagnostics.html">Diagnostics</a>
</li>
<li>
  <a href="play_with_seitl.html">Modelling interlude</a>
</li>
<li>
  <a href="mcmc_and_model_comparison.html">Model comparison</a>
</li>
<li>
  <a href="pmcmc.html">Particle MCMC</a>
</li>
<li>
  <a href="ABC.html">ABC</a>
</li>
<li>
  <a href="further_methods.html">Further methods</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="contact.html">
    <span class="fa fa-envelope-o"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/sbfnk/mfiidd">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Full example: particle MCMC</h1>

</div>


<div id="run-a-particle-filter" class="section level1">
<h1>Run a particle filter</h1>
<p>The particle filter returns a Monte-Carlo estimate of the log-likelihood and, as every Monte-carlo estimate, its precision depends on the number of particles.</p>
<p>If you have too few particles then you will have a highly variable estimate of the log-likelihood and this will make the exploration of the likelihood surface quite imprecise. In addition, you might experience particle depletion (if you don’t know what that means just try to run a particle filter with a single particle).</p>
<p>If you have too many particles, then you will have an accurate estimate of your log-likelihood but it will be very time consuming so inefficient in practice.</p>
<p>In addition, the variability of the estimate might also depend on the region of the parameter space. For instance, in some region you might experience significant variability with 100 particles whereas in another region it might be fairly stable.</p>
<p>You can now return to the <a href="pmcmc.html#calibrate-the-number-of-particles">practical</a> and try to think on a way to calibrate the number of particles.</p>
</div>
<div id="calibrate-the-number-of-particles" class="section level1">
<h1>Calibrate the number of particles</h1>
<p>Ideally we want enough particles to obtain a fairly stable estimate of the log-likelihood in a reasonable computational time.</p>
<p>A simple calibration approach consists in plotting the mean, standard deviation and computational time of the log-likelihood estimate as a function of the number of particles. Although several parameters can be tested, we will use a <code>theta</code> close to the mean posterior estimate of the deterministic fit since it is likely to be the region of the parameter space we want to explore with the pMCMC.</p>
<pre class="r"><code>knitr::read_chunk(here::here(&quot;scripts&quot;, &quot;snippets&quot;, &quot;calibrate-smc.r&quot;))</code></pre>
<p>We ran this calibration algorithm and obtained the following results:</p>
<p><img src="figure/pmcmc_solution/plot-calibration-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>We can see how, with increasing number of particles, the standard deviation of the log-likelihood estimate decreases whilst the run time increases. Ideally we want to balance between a stable enough estimate of the log-likelihood to ensure good mixing and computation time. Various studies have shown that this is broadly the case at values of the standard deviation between 1 and 3 (see, e.g. <a href="https://doi.org/10.48550/arXiv.1709.04196">Farnhead and Küsch, section 7.2.1</a>). We can see that this happens somewhere around 100 particles in our example (indicated by a dashed red line). We can also see that the computational time increases linearly with the number of particles. Using 128 particles may be optimal if we want a stable estimator while minimising the computational time, but would take 2.3 days on the computer where this experiment was run to run the particle filter 10,000 times. Since every step of the pMCMC requires to run a particle filter, that means that it will take the same amount of time to generate 10,000 samples of the pMCMC. This is why you will often need a scientific computing cluster to run a pMCMC and make extensive use of parallelisation.</p>
<p>You can now return to the <a href="pmcmc.html#run-pmcmc">practical</a> and set your pMCMC with 128 particles.</p>
</div>
<div id="setting-the-pmcmc" class="section level1">
<h1>Setting the pMCMC</h1>
<pre class="r"><code>knitr::read_chunk(here::here(&quot;scripts&quot;, &quot;snippets&quot;, &quot;set-pmcmc.r&quot;))</code></pre>
<p>You can now go back to the <a href="pmcmc.html#run-a-pmcmc">practical</a> and try to run pMCMC with those settings.</p>
</div>
<div id="analyse-a-pmcmc-with-16-particles" class="section level1">
<h1>Analyse a pMCMC with 16 particles</h1>
<p>Here is an example of analysis of our 8 chains of 3000 iterations with 16 particles.</p>
<pre class="r"><code># load traces
data(pmcmcSeit4lInfoPrior)

# combine into a `mcmc.list` object
library(&quot;coda&quot;)
trace16 &lt;- mcmc.list(lapply(pmcmcSeit4lInfoPrior16, function(chain) {
    mcmc(chain$trace)
}))

# acceptance rate is way below the optimal 23%
1 - rejectionRate(trace16)
##           R_0         D_lat         D_inf         alpha         D_imm 
##    0.05130877    0.05130877    0.05130877    0.05130877    0.05130877 
##           rho      logPrior logLikelihood    logDensity 
##    0.05130877    0.05130877    0.05130877    0.05130877

# Let&#39;s have a look at the traces
library(&quot;lattice&quot;)
xyplot(trace16)</code></pre>
<p><img src="figure/pmcmc_solution/analyse-n8-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>We can see that in several chains the sampler gets “stuck”, rendering our inference and any resulting calculations (e.g. of ESS) highly unreliable.</p>
<p>With so few particles, the likelihood estimate is very noisy and the exploration of the parameter space is not efficient. That said, the theoretical properties of the pMCMC guarantee that the chain will converge to the true posterior, even with 1 particle. Of course, this will take a lot of iterations so in practice it might be more efficient to spend more time computing the likelihood (i.e. having more particles) in order to reduce the number of iterations.</p>
<p>You can now return to the <a href="pmcmc.html#analyse-a-pmcmc-with-128-particles">practical</a> and analyse a pMCMC with much more particles that we ran for you.</p>
</div>
<div id="analyse-a-pmcmc-with-128-particles" class="section level1">
<h1>Analyse a pMCMC with 128 particles</h1>
<p>Here is an example of analysis of our 8 chains of 3000 iterations with 128 particles.</p>
<pre class="r"><code>
# combine into a `mcmc.list` object
library(&quot;coda&quot;)
trace &lt;- mcmc.list(lapply(pmcmcSeit4lInfoPrior128, function(chain) {
    mcmc(chain$trace)
}))

# acceptance rate is near optimal
1 - rejectionRate(trace)
##           R_0         D_lat         D_inf         alpha         D_imm 
##     0.1988996     0.1988996     0.1988996     0.1988996     0.1988996 
##           rho      logPrior logLikelihood    logDensity 
##     0.1988996     0.1988996     0.1988996     0.1988996

# Note that the combined ESS is 2 times higher than with 16 particles
effectiveSize(trace)
##           R_0         D_lat         D_inf         alpha         D_imm 
##      520.0792      711.3816      861.4712      709.9831      712.1860 
##           rho      logPrior logLikelihood    logDensity 
##      718.5242     1056.8813      771.8884      700.4612

# Let&#39;s have a look at the traces
library(&quot;lattice&quot;)
xyplot(trace)</code></pre>
<p><img src="figure/pmcmc_solution/analyse-n64-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>As in the analysis with 16 particles, the burn-in period is relatively short. However, with 128 particles the chains mix much better. We can carefully choose the burn-in period with <code>plotEssBurn</code>.</p>
<pre class="r"><code># Actually, it looks like no burn-in is needed:
plotEssBurn(trace)</code></pre>
<p><img src="figure/pmcmc_solution/analyse-n128-burn-1.png" width="960" style="display: block; margin: auto;" /></p>
<pre class="r"><code>
# What about autocorrelation?
acfplot(x = trace, lag.max = 50)</code></pre>
<p><img src="figure/pmcmc_solution/analyse-n128-burn-2.png" width="960" style="display: block; margin: auto;" /></p>
<pre class="r"><code>
# Let&#39;s keep 1 iteration every 20
traceThin128 &lt;- burnAndThin(trace, thin = 20)

# Let&#39;s plot the posterior densities
densityplot(x = traceThin128)</code></pre>
<p><img src="figure/pmcmc_solution/analyse-n128-burn-3.png" width="960" style="display: block; margin: auto;" /></p>
<p>All 8 chains seems to have converged to the same posterior, which are smoother than with 16 particles. Let’s compare the combined posterior densities with that obtained with 16 particles</p>
<pre class="r"><code>plotPosteriorDensity(list(n16 = trace16, n128 = traceThin128))</code></pre>
<p><img src="figure/pmcmc_solution/n128-n16-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><strong>Although the posterior distributions are similar, those with 128 particles are smoother and more representative thanks to higher ESS.</strong> Note the different location of the mode of <span class="math inline">\(R_0\)</span>, which is shifted to the left with 16 particles. This is because 1 of the 8 chains with 16 particles shows a posterior with much lower <span class="math inline">\(R_0\)</span> (see figure above), whereas the remaining 4 seems to have converged to the same distribution as the pMCMC with 128 particles.</p>
<p>Finally, note that the log-likelihood is overestimated with 16 particles, which can be problematic for model selection as we would overestimate the fit of the model.</p>
<p><strong>Overall, this analysis confirms that the pMCMC works even with 16 particles but that it will require much more iterations to achieve the same posterior as the pMCMC with 128 particles.</strong> Although the latter takes more time at each iteration, it provides more better samples on short-runs. A good strategy is therefore to run many short chains in parallel with 128 particles. The chains start at different <code>initTheta</code> near the mode of the deterministic posterior, and are then combined to increase the overall ESS.</p>
<p>You can now return to the <a href="pmcmc.html#stochastic-vs-deterministic-fit">practical</a> and proceed to the last section of this session.</p>
</div>
<div id="stochastic-vs-deterministic-fit" class="section level1">
<h1>Stochastic vs deterministic fit</h1>
<p>Here we compare the combined traces of the deterministic SEIT4L model (2 chains of 50000 iterations) with those obtained with the stochastic version (8 chains of <span class="math inline">\(3000\)</span> iterations). Both analysis have assumed informative priors for <span class="math inline">\(D_\mathrm{lat}\)</span> and <span class="math inline">\(D_\mathrm{inf}\)</span> .</p>
<pre class="r"><code># load, burn and thin the deterministic fit

# create mcmc object
library(&quot;coda&quot;)
data(mcmcTdcDeterLongRun)
trace1 &lt;- mcmc(mcmcSeit4lInfoPriorTheta1$trace)
trace2 &lt;- mcmc(mcmcSeit4lInfoPriorTheta2$trace)

# combine in a mcmc.list
trace &lt;- mcmc.list(trace1, trace2)

# burn and thin as the chain with uniform prior (see above sections)
traceDeter &lt;- burnAndThin(trace, burn = 5000, thin = 40)

# compare posterior density
plotPosteriorDensity(list(deter = traceDeter, sto = traceThin128))</code></pre>
<p><img src="figure/pmcmc_solution/sto-deter-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Overall, the posterior distributions are quite different. This is especially true for <span class="math inline">\(R_0\)</span> and <span class="math inline">\(D_{imm}\)</span>. In addition, the discrepancy in the posterior distribution of the log-likelihood seems to indicate that the stochastic model fits much better. We can quantify this by computing the DIC of the stochastic SEIT4L model.</p>
<pre class="r"><code># combine all traces in a data frame
library(&quot;dplyr&quot;)
library(&quot;purrr&quot;)
traceCombined &lt;- bind_rows(map(traceThin128, as.data.frame))

# take the mean of theta
thetaBar &lt;- colMeans(traceCombined[seit4lStoch$thetaNames])
print(thetaBar)
##        R_0      D_lat      D_inf      alpha      D_imm        rho 
##  8.8950674  1.6844288  2.2466477  0.5051892 12.5774892  0.7084080

# compute its log-likelihood
initState &lt;- c(S = 279, E = 0, I = 2, T1 = 3, T2 = 0, T3 = 0, T4 = 0, L = 0, Inc = 0)
logLikeThetaBar &lt;- my_particleFilter(seit4lStoch, thetaBar, initState, data = fluTdc1971,
    nParticles = 128)
print(logLikeThetaBar)
## [1] -113.8204

logLikeThetaBarDeter &lt;- dTrajObs(seit4lDeter, thetaBar, initState, data = fluTdc1971,
    log = TRUE)
print(logLikeThetaBarDeter)
## [1] -176.2187

# and its deviance
dThetaBar &lt;- -2 * logLikeThetaBar
print(dThetaBar)
## [1] 227.6408

# the effective number of parameters
pD &lt;- var(-2 * traceCombined$logLikelihood)/2
print(pD)
## [1] 6.883178

# and finally the DIC
dic &lt;- dThetaBar + 2 * pD
print(dic)
## [1] 241.4072</code></pre>
<p>In the previous session, we found that the DIC of the deterministic SEIT4L model was equal to 265. The difference of 20 indicates that the stochastic model should strongly be preferred to the deterministic model.</p>
<p>We can visually check this result by plotting the posterior fit of each model:</p>
<pre class="r"><code># take the mean posterior estimates of the deterministic model
x &lt;- summary(traceDeter)
thetaBarDeter &lt;- x$statistics[seit4lDeter$thetaNames, &quot;Mean&quot;]

plotFit(seit4lStoch, thetaBar, initState, data = fluTdc1971, nReplicates = 1000)</code></pre>
<p><img src="figure/pmcmc_solution/sto-deter-fit-1.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plotFit(seit4lDeter, thetaBarDeter, initState, data = fluTdc1971, nReplicates = 1000)</code></pre>
<p><img src="figure/pmcmc_solution/sto-deter-fit-2.png" width="480" style="display: block; margin: auto;" /></p>
<p>Despite the fact that the deterministic model seems to better capture the first peak of the epidemic, the stochastic model better explains the variability of the observed time-series. In particular, the 95% CI of the stochastic model captures almost all the observed data points, even during the first peak.</p>
<p>You’ve already finished? Why not <a href="pmcmc.html#going-further">go further</a>?</p>
</div>

<script data-goatcounter="https://mfiidd.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
