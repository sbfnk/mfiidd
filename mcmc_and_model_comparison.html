<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>MCMC and model comparison: the deterministic SEIT(4)L models</title>

<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-6.4.0/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.0/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="mfiidd.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">MFIIDD 2023</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="introduction.html">Introduction</a>
</li>
<li>
  <a href="mcmc.html">MCMC</a>
</li>
<li>
  <a href="mcmc_diagnostics.html">Diagnostics</a>
</li>
<li>
  <a href="play_with_seitl.html">Modelling interlude</a>
</li>
<li>
  <a href="mcmc_and_model_comparison.html">Model comparison</a>
</li>
<li>
  <a href="pmcmc.html">Particle MCMC</a>
</li>
<li>
  <a href="ABC.html">ABC</a>
</li>
<li>
  <a href="further_methods.html">Further methods</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="contact.html">
    <span class="fa fa-envelope-o"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/sbfnk/mfiidd">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">MCMC and model comparison: the deterministic SEIT(4)L models</h1>

</div>


<div id="objectives" class="section level1">
<h1>Objectives</h1>
<p>As you should have noticed in the previous session, several simulations are required to assess the dynamics of a stochastic model against a single one for a deterministic model. Similarly, in the next session you will see that <strong>fitting a stochastic model is also computationally much more intensive than fitting a deterministic model</strong>. Because of this, we want to avoid wasting simulation runs, which could happen for two reasons:</p>
<ul>
<li>If you initialise the MCMC with a <code>initTheta</code> far from the region of high posterior density, <strong>the chain might take a long time to reach this region of interest</strong> and you will have to burn a lot of iterations.</li>
<li>If the covariance matrix of the Gaussian proposal is very different from the posterior, this will result in <strong>poor mixing and sub-optimal acceptance rates</strong> and you will have to run your chain for longer (or thin it).</li>
</ul>
<p>In this context, it can be useful to first run a MCMC on the deterministic model, which can be simulated faster, and learn from the output of this chain to initialise the chain for the fit of the stochastic model. <strong>The rationale for this approach is that the deterministic model is an approximation of the stochastic model and should capture, in most cases, the same dynamics.</strong></p>
<p>So in this session you will:</p>
<ol style="list-style-type: decimal">
<li>Fit the deterministic SEITL and SEIT4L models to the Tristan da Cunha outbreak. This will prepare you for the next session.</li>
<li>Learn on issues that arise when fitting a lot of parameters and how you can try to solve them.</li>
<li>Compare both models and assess whether the best one actually provides a good fit of the data.</li>
</ol>
<p>To save time, half of the group will fit the deterministic SEITL model and the other half will fit the deterministic SEIT4L model. In the rest of the session, although most of the examples will refer to the SEITL model, the same commands work for the SEIT4L model, i.e.Â using <code>data(seit4lDeter)</code> instead of <code>data(seitlDeter)</code>.</p>
</div>
<div id="run-a-mcmc" class="section level1">
<h1>Run a MCMC</h1>
<p>Here you could use the function <code>my_mcmcMh</code> that you have already coded but it might result in poor acceptance rates since the models have 6 parameters to be estimated. Because of this, using the adaptive MCMC might be a better choice.</p>
<p><strong>Take 10 minutes to have a look at the adaptive MCMC implemented in the function <code>mcmcMh</code>.</strong> As you can see, this function takes similar arguments as your function <code>my_mcmcMh</code>: <code>target</code>, <code>initTheta</code>, <code>proposalSd</code> and <code>nIterations</code>, plus several new ones that control the adaptive part of the algorithm and that you can learn about via the help page of <code>mcmcMh</code>.</p>
<p>Note also that this function returns a list that contains, in addition to the trace, the acceptance rate and the empirical covariance matrix of the posterior. The latter will be useful for improving the covariance matrix of the Gaussian proposal in the next session, when we will fit the stochastic model.</p>
<p>The first step is to code a wrapper function to evaluate the posterior at a given <code>theta</code> value. Here again, you could wrap the function <code>my_dLogPosterior</code> that you have already coded and pass it to <code>mcmcMh</code> so that it returns samples of <code>theta</code> from the posterior distribution.</p>
<p>However, as you will see at the end of this session, in order to be able to compare different models we also need to track the log-likelihood of the sampled <code>theta</code>. Although this could be done by running <code>dTrajObs</code> on each returned <code>theta</code>, you might remember that the log-likelihood is actually computed in <code>my_dLogPosterior</code> so we could just use it. This is exactly what the function <code>dLogPosterior</code> does for you.</p>
<p><strong>Take 5 minutes to look at the code of <code>dLogPosterior</code>.</strong> As you can see, this function takes similar arguments as your function <code>my_dLogPosterior</code>: <code>fitmodel</code>, <code>theta</code>, <code>initState</code> and <code>data</code>, plus one called <code>margLogLike</code>, which takes the function that will compute the log-likelihood of <code>theta</code>. The last argument (<code>...</code>) is called dot-dot-dot and adds some flexibility to the definition of <code>margLogLike</code>.</p>
<p>For instance, we have seen that for a deterministic model, the likelihood was returned by <code>dTrajObs</code>. However, <code>margLogLike</code> expects a function that returns the log-likelihood, which can be obtained by passing <code>log = TRUE</code> to <code>dTrajObs</code>. To do so, we can use the dot-dot-dot argument of <code>dLogPosterior</code>, which allows you to pass any extra argument to the function assigned to <code>margLogLike</code>.</p>
<p>The function <code>dLogPosterior</code> returns a list of two elements:</p>
<ol style="list-style-type: decimal">
<li><code>logDensity</code>: the log of the posterior density</li>
<li><code>trace</code>: a vector that contains, among other things, <code>theta</code> and its <code>logLikelihood</code>. All this information will be collected in the <code>trace</code> data frame returned by <code>mcmcMh</code>.</li>
</ol>
<p><strong>Now, take 15 minutes to prepare all the inputs to be able to run <code>mcmcMh</code> and fit your model.</strong> You should proceed as follows:</p>
<pre class="r"><code># wrapper for posterior
my_posteriorTdc &lt;- function(theta){

    my_fitmodel &lt;- # INSERT HERE
    my_initState &lt;- # INSERT HERE

        return(dLogPosterior(fitmodel = my_fitmodel,
                             theta = theta,
                             initState =  my_initState,
                             data = fluTdc1971,
                             margLogLike  =  dTrajObs,
                             log = TRUE))

}

# theta to initialise the MCMC
initTheta &lt;- # INSERT HERE

# diagonal elements of the covariance matrix for the Gaussian proposal
# Must be in the same order as initTheta or named
proposalSd &lt;- # INSERT HERE


# lower and upper limits of each parameter (must be named vectors)
lower &lt;- # INSERT HERE
upper &lt;- # INSERT HERE

# number of iterations for the MCMC
nIterations &lt;- # INSERT HERE

# additional parameters for the adaptive MCMC, see ?mcmcMh for more details
adaptSizeStart &lt;- # INSERT HERE
adaptSizeCooling &lt;- # INSERT HERE
adaptShapeStart &lt;- # INSERT HERE</code></pre>
<p>If you have trouble filling some of the empty bits, have a look at our <a href="mcmc_and_model_comparison_example.html#setting-the-mcmc">example</a>.</p>
<p>Then you should be able to run <code>mcmcMh</code>:</p>
<pre class="r"><code>knitr::read_chunk(here::here(&quot;scripts&quot;, &quot;snippets&quot;, &quot;run-mcmc.r&quot;))</code></pre>
<p>You should see some information printed as the chain runs: acceptance rate, state of the chain, log-likelihood etc. <strong>In particular, what can you say about the evolution of the acceptance rate?</strong></p>
<p>Note that we have set the number of iterations to 5000. This is a benchmark and if your laptop is quite slow you might want to perform fewer iterations. Here the objective is to have a short - preliminary - run to calibrate your adaptive parameters before running a longer chain.</p>
<p><strong>Take 10 minutes to change the parameters controlling the adaptive part of the MCMC and look at the effect on the acceptance rate.</strong> Try to find a âgoodâ combination of these parameters so that the acceptance rate is near the optimal value of 23% (actually, the algorithm efficiency remains high whenever the acceptance rate is between about 0.1 and 0.6 so any value in between is OK). If you canât find one, look at our <a href="mcmc_and_model_comparison_example.html#setting-the-mcmc">example</a>.</p>
</div>
<div id="short-run-analysis" class="section level1">
<h1>Short run analysis</h1>
<p><strong>Now itâs time to use what youâve learned in the previous session to analyse your MCMC outputs using the <code>coda</code> package.</strong></p>
<pre class="r"><code>library(&quot;coda&quot;)</code></pre>
<p>Didnât manage to run <code>mcmcMh</code>? Just use the results from our <a href="mcmc_and_model_comparison_example.html#run-mcmc">example</a>.</p>
<p>Note that because there are more than 6 parameters to look at, the <code>coda</code> functions used previously to plot the traces, densities and autocorrelations might not be optimal for laptop screens (the axis labels take too much space). Fortunately, <code>coda</code> has another set of functions to make the plots more compact:</p>
<ul>
<li><code>xyplot</code> for the trace.</li>
<li><code>densityplot</code> for the density.</li>
<li><code>acfplot</code> for the autocorrelation.</li>
</ul>
<p>You can find a grouped documentation for these three functions by typing <code>?acfplot</code>.</p>
<p><strong>Take 15 minutes to analyse the trace returned by <code>mcmcMh</code>.</strong> Remember that, with the notation above, the trace can be accessed by <code>my_mcmcTdc$trace</code> and then converted to a <code>mcmc</code> object so that the <code>coda</code> functions recognize it:</p>
<pre class="r"><code># convert to a mcmc object for coda
my_trace &lt;- mcmc(my_mcmcTdc$trace)
# plot the trace
library(&quot;lattice&quot;)  ## for xyplot
xyplot(my_trace)</code></pre>
<ul>
<li>Are you surprised by the trace of the log-prior?</li>
<li>Determine what burning and thinning would be appropriate for your trace.</li>
<li>Compare the thinned and unthinned traces (e.g.Â posterior estimates, density). Do you think it is important to thin?</li>
</ul>
<p>If you are not sure about how to analyse your trace, have a look at our <a href="mcmc_and_model_comparison_example.html#short-run-analysis">example</a>.</p>
</div>
<div id="long-run-analysis" class="section level1">
<h1>Long run analysis</h1>
<p>You should have noticed that the effective sample size (ESS) is quite small (&lt;100) for your preliminary run. By contrast, the ESS was much higher for the SIR model of the previous session with a similar number of iterations. However, this model has only 2 parameters against 6 for the SEIT(4)L models. Intuitively, the more parameters you have the bigger is the parameter space and the longer it takes to the MCMC algorithm to explore it. This is why we need to run a much longer chain to achieve a good ESS (~1000).</p>
<p>To save time, we have run 2 chains of 100,000 iterations starting from different initial <code>theta</code> values:</p>
<pre class="r"><code>knitr::read_chunk(here::here(&quot;scripts&quot;, &quot;snippets&quot;, &quot;theta-init.r&quot;))</code></pre>
<p>Each chain took 4 hours to run on a single-CPU and can be loaded as follows:</p>
<pre class="r"><code>data(mcmcTdcDeterLongRun)
# this should load 2 objects in your environment: mcmcSeitlTheta1 and
# mcmcSeitlTheta2. Each one is a list of 3 elements returned by mcmcMh
names(mcmcSeitlTheta1)
## [1] &quot;trace&quot;           &quot;acceptanceRate&quot;  &quot;covmatEmpirical&quot;
# the trace contains 9 variables for 100000 iterations
dim(mcmcSeitlTheta1$trace)
## [1] 50000     9
# let&#39;s have a look at it
head(mcmcSeitlTheta1$trace)
##           R_0    D_lat    D_inf     alpha    D_imm       rho  logPrior
## [1,] 2.000000 2.000000 2.000000 0.8000000 16.00000 0.8500000 -12.81448
## [2,] 2.000000 2.000000 2.000000 0.8000000 16.00000 0.8500000 -12.81448
## [3,] 1.722749 1.481383 1.186734 0.8390849 11.34142 0.8183184 -12.81448
## [4,] 1.722749 1.481383 1.186734 0.8390849 11.34142 0.8183184 -12.81448
## [5,] 1.722749 1.481383 1.186734 0.8390849 11.34142 0.8183184 -12.81448
## [6,] 1.722749 1.481383 1.186734 0.8390849 11.34142 0.8183184 -12.81448
##      logLikelihood logDensity
## [1,]     -445.7795  -458.5939
## [2,]     -445.7795  -458.5939
## [3,]     -421.8376  -434.6520
## [4,]     -421.8376  -434.6520
## [5,]     -421.8376  -434.6520
## [6,]     -421.8376  -434.6520</code></pre>
<p><strong>Take 15 minutes to analyse both chains together.</strong></p>
<p><strong>Hint</strong>: the function <code>mcmc.list</code> of <code>coda</code> can be used to combine several <code>mcmc</code> objects into a <code>mcmc.list</code> object. Diagnostic functions which act on <code>mcmc</code> objects may also be applied to <code>mcmc.list</code> objects. In general, the chains will be combined, if this makes sense, otherwise the diagnostic function will be applied separately to each chain in the list.</p>
<ul>
<li>Did the chains converge to the same distribution?</li>
<li>What advantage can you see in combining several independent chains?</li>
<li>Record the mean, median and 95% credible intervals of each parameter.</li>
</ul>
<p>Finally, you should also assess the fit of your model by using the function <code>plotPosteriorFit</code>. This function takes a sample of <code>theta</code> from the trace, simulates some observations and plot them against the data, thus allowing you to assess the fit. <strong>Take 10 minutes to look at the documentation of this function and assess your fit, in particular try the different options for the argument <code>posterior.summary</code>.</strong></p>
<p>Once again, if you are not sure how to complete these steps, have a look at our <a href="mcmc_and_model_comparison_example.html#long-run-analysis">example</a>.</p>
</div>
<div id="correlations" class="section level1">
<h1>Correlations</h1>
<p>You can check at correlations between parameters using the function <code>levelplot</code> from the <code>coda</code> package. Note however that this function doesnât accept <code>mcmc.list</code> objects.</p>
<p><strong>Which parameters are strongly correlated? Can you explain why?</strong></p>
<!-- You should have observed that, in contrast to the other parameters, the basic reproduction number ($R_0$) and the infectious period ($D_{inf}$) have wide 95% credible intervals.  -->
<p>As previously stated, both the latent and infectious period have been estimated to be around 2 days in empirical studies and are unlikely to last more than 5 days.</p>
<p><strong>Are your estimates in agreement with these previous studies? If not, did your approach take into account this prior information when fitting your model?</strong></p>
<p>Modify <code>seitlDeter</code> to account this prior information and re-run a short MCMC for 5000 iterations. <strong>Can you notice a difference in the posterior?</strong></p>
<p>A solution can be found <a href="mcmc_and_model_comparison_example.html#correlations">here</a>.</p>
</div>
<div id="informative-priors" class="section level1">
<h1>Informative priors</h1>
<p>In order to take into account the results of previous empirical studies, we have re-run both chains for <span class="math inline">\(10^5\)</span> iterations with the following informative priors:</p>
<ul>
<li><span class="math inline">\(D_{lat}\sim\mathcal{N}(\mu=2,\sigma=1)\)</span></li>
<li><span class="math inline">\(D_{inf}\sim\mathcal{N}(\mu=2,\sigma=1)\)</span></li>
</ul>
<pre class="r"><code>names(mcmcSeitlInfoPriorTheta1)
## [1] &quot;trace&quot;           &quot;acceptanceRate&quot;  &quot;covmatEmpirical&quot;
names(mcmcSeitlInfoPriorTheta2)
## [1] &quot;trace&quot;           &quot;acceptanceRate&quot;  &quot;covmatEmpirical&quot;</code></pre>
<p><strong>Take 15 minutes to perform the same analysis as before and compare the posteriors with and without informative priors. Which parameters have significantly different posteriors? Does that make sense to you?</strong></p>
<p>A solution can be found <a href="mcmc_and_model_comparison_example.html#informative-priors">here</a>.</p>
</div>
<div id="model-comparison" class="section level1">
<h1>Model comparison</h1>
<p>We can compare the SEITL and SEIT4L models using the deviance information criterion (<a href="http://en.wikipedia.org/wiki/Deviance_information_criterion">DIC</a>).</p>
<p>The deviance of a parameter set <span class="math inline">\(\theta\)</span> is defined as <span class="math display">\[
D(\theta)=-2\log(p(y|\theta)) + C
\]</span> where <span class="math inline">\(p(y|\theta)\)</span> is the likelihood of the data given <span class="math inline">\(\theta\)</span> and <span class="math inline">\(C\)</span> is a constant that will cancel out when comparing two models.</p>
<p>The DIC can be computed as <span class="math display">\[
\mathrm{DIC}=D(\bar\theta) + 2p_D
\]</span> where <span class="math inline">\(\bar\theta\)</span> is the mean of <span class="math inline">\(\theta\)</span> with respect to the posterior distribution, and <span class="math inline">\(p_D\)</span> is the effective number of parameters, which is approximately equal to: <span class="math display">\[p_D=\frac{1}{2}\hat{\mathrm{var}}(D(\theta))\]</span> that is half of the variance of the deviance with respect to the posterior distribution.</p>
<p><strong>The idea is that models with smaller DIC should be preferred to models with larger DIC.</strong> Models are penalized both by the value of <span class="math inline">\(D(\bar\theta)\)</span>, which favors a good fit, but also by the effective number of parameters <span class="math inline">\(p_D\)</span>. Since <span class="math inline">\(D(\bar\theta)\)</span> will decrease as the number of parameters in a model increases, the <span class="math inline">\(p_D\)</span> term compensates for this effect by favouring models with a smaller number of effective parameters.</p>
<p>Compute the DIC for your model. A solution is provided <a href="mcmc_and_model_comparison_example.html#model-selection">here</a>.</p>
<p><strong>Now, itâs time to compare the DIC of the SEITL and SEIT4L model. Which model should be preferred? Is the difference substantial?</strong></p>
<p>You can have a look at the <a href="http://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-dic/">MRC FAQ</a> on DIC to decide which model is the best.</p>
</div>
<div id="posterior-predictive-checks" class="section level1">
<h1>Posterior predictive checks</h1>
<p>The DIC can only tell you which one of two or more models fit the data best; <strong>it doesnât give you any information on whether the best model provides actually a good fit to the data</strong> <em>overall</em> (in the kingdom of the blind, the one-eyed man is king).</p>
<p>One way to assess the overall quality of a fit is to perform <em>posterior predictive checks</em>. It works by choosing a test statistic of the data (i.e., the peak, final size, etc.) and running many model replicates from parameter values that have been sampled from the posterior.</p>
<p>One then tests how often in the simulations the test statistic takes a value as extreme or more extreme than the value it takes in the data. This allows us to compute a p-value (a frequentist quantity, strictly speaking).</p>
<p>If the data come out as an extreme case of model output (that is, the chosen test statistic or a more extreme value only comes up very rarely in the simulations), it would indicate that the model is not a good fit to the data.</p>
<p>Letâs look at the maximum in the data</p>
<pre class="r"><code>max(fluTdc1971$obs)</code></pre>
<pre><code>## [1] 47</code></pre>
<p>Now, write a function that takes as argument a data frame of parameter samples, the number of trajectories to evaluate, the model, initial conditions and a data set, and which returns the Bayesian p-value of the test statistic in the data with respect to the posterior samples.</p>
<pre class="r"><code># This is a function that takes 4 arguments:
# - trace, a data frame containing samples from the posterior
#   distribution, one column per parameter 
# - nSamples, the number of samples to take
# - fitmodel, the model we use to generate replicates
# - initState, the initial state
# - data, the data set we have fit the model to
# It should return the two-sided p-value for the maximal observation
# in the data with respect to the model.
my_postPredCheck &lt;- function(trace, nSamples, fitmodel, initState, data) {

    # calculate maximum in obs column of data
    
    # draw nSamples random numbers between 1
    # and nSamples using the `sample` function

    # initialise vector of model maxima
    
    ## start for() loop over sampled numbers

        # get row from trace corresponding to sampled number

        # use rTrajObs to generate observation trajectory using row
        # from trace

        # calculate maximum in model and add to vector of model maxima

    ## end for() loop

    # calculate 2-sided p-value, that is the proportion of elements of
    # maxModel which are either greater or equal or less or equal
    # (whichever is less) and  multiply by 2 (because it is a 2-sided
    # test)

    # return two-sided p-value
}</code></pre>
<p>If you have trouble filling any of the empty bits, have a look at our <a href="our_ppc_insert.html">more guided example</a>.</p>
<p>Now, do a posterior predictive check using one of the mcmc runs analysed above. For example,</p>
<pre class="r"><code>initState &lt;- c(S = 279, E = 0, I = 2, T = 3, L = 0, Inc = 0)
my_postPredCheck(
  trace = mcmcSeitlTheta1$trace[, seitlDeter$thetaNames],
  nSamples = 100,
  fitmodel = seitlDeter,
  initState = initState,
  data = fluTdc1971
)</code></pre>
<pre><code>## [1] 0.1</code></pre>
<p>Of course, this is the outcome of a random draw (100 samples in this case), so it will give a different result every time. With more samples, you could improve the estimate.</p>
<p>Does the best model yield to a âbetterâ (higher) p-value?</p>
<p>The corresponding outputs of <code>mcmcMh</code> were loaded with <code>data(mcmcTdcDeterLongRun)</code> and should already be in your <strong>R</strong> environment:data(mcmcTdcDeterLongRun)</p>
</div>

<script data-goatcounter="https://mfiidd.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
