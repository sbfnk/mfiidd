---
title: "Particle MCMC"
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(cache = TRUE, fig.path = "figure/pmcmc/", cache.path = "cache/pmcmc/", fig.cap = "", fig.align = "center", message = FALSE, tidy = TRUE, warning = FALSE)
```

```{r fitR, echo=FALSE, cache=FALSE, results="hide"}
library(fitR)
set.seed(1234)
```
[Lecture slides](slides/smc.pdf)

# Objectives

The aim of this session is to learn how you can fit a stochastic model using the particle MCMC Metropolis-Hastings algorithm (pMCMC). As you saw in the lecture, fitting a model with a likelihood approach requires:

* To explore the parameter space efficiently
* To evaluate the likelihood at a given parameter 

In the [previous session](mcmc_and_model_comparison.html), you have used the function `mcmcMh` to explore the parameter space with the Metropolis-Hastings algorithm and the function `dTrajObs` to evaluate the likelihood for your deterministic model. In order to fit a stochastic model, we can still use the `mcmcMh` function to explore the parameter space efficiently but we need to use a new function to evaluate the likelihood. This new function is called a particle filter. 

In this session you will:

1. code a particle filter to estimate the likelihood of a stochastic model
2. learn how to calibrate the number of particles
3. fit the stochastic SEIT4L model to the Tristan da Cunha outbreak with pMCMC

# Code a particle filter

Before running pMCMC, we need to code a particle filter in order to evaluate the log-likelihood of the data at a given proposed `theta`.

Below you can find the skeleton of such a function. We have inserted comments at each step of the algorithm. If you are struggling at any point, follow the link below the code for a more guided example.

```{r skeleton-PF, eval=FALSE, tidy=FALSE}
# This is a function that takes four parameters:
# - fitmodel: a fitmodel object
# - theta: named numeric vector. Values of the parameters for which the marginal log-likelihood is desired.
# - initState: named numeric vector. Initial values of the state variables.
# - data: data frame. Observation times and observed data.
# The function returns the value of the marginal log-likelihood
my_particleFilter <- function(fitmodel, theta, initState, data, nParticles) {

    ## Initialisation of the algorithm
    # Initialise the state and the weight of your particles

    ## Start for() loop over observation times

        # Resample particles according to their weights
        # You can use the `sample() function of R

        ## Start for() loop over particles

            # Propagate the particle from current observation time to the next one
            # using the function `fitmodel$simulate`

            # Weight the particle with the likelihood of the observed data point
            # using the function `fitmodel$dPointObs`

        ## End for() loop over particles

    ## End for() loop over observation times

    ## Compute and return the marginal log-likelihood
    # sum of the log of the mean of the weights at each observation time

}
```

```{r load-PF, echo=FALSE}
source(here::here("scripts", "snippets", "our-smc.r"))
```

If you have trouble filling any of the empty bits, have a look at our [more guided example](smc_example.html).

# Run a particle filter

Try to run your particle filter with the following inputs:

```{r try-PF, collapse=TRUE}
# load seit4lStoch
data(models)

# load data
data(fluTdc1971)

# theta close to the mean posterior estimate of the deterministic SEIT4L model
theta <- c("R_0" = 7, "D_lat" = 1, "D_inf" = 4, "alpha" = 0.5, "D_imm" = 10, "rho" = 0.65)

# init state as before
initState <- c("S" = 279, "E" = 0, "I" = 2, "T1" = 3, "T2" = 0, "T3" = 0, "T4" = 0, "L" = 0, "Inc" = 0)

# run the particle filter with 20 particles
my_particleFilter(seit4lStoch, theta, initState, data = fluTdc1971, nParticles = 20)
```

Does your particle filter return the same value for the marginal log-likelihood? Can you explain why?

What can you notice when you:

* Run several replicates with the same number of particles
* Increase the number of particles
* Decrease the number of particles (try with one particle)
* Change `theta`

# Calibrate the number of particles

Can you think of and implement an algorithm to calibrate the number of particles? 

Compare your approach with [ours](pmcmc_example_solution.html#calibrate-the-number-of-particles) and determine an optimal number of particles.

# Run pMCMC

You can now write a new wrapper for the function `dLogPosterior` that will take `margLogLike = my_particleFilter` as argument. This wrapper will then be passed to `mcmcMh`.

Note that the function `dLogPosterior` doesn't have a `nParticles` argument so you might wonder how to specify it to `my_particleFilter`? Have a look at the documentation of `dLogPosterior`. You should notice the `...` argument, which allows you to pass any extra argument to the function `margLogLike`.

You have probably noticed that running `my_particleFilter` is time consuming. As we mentioned in the [previous session](mcmc_and_model_comparison.html#objectives) this can lead to a waste of time and computational resources if you initialise the pMCMC with parameter values far from the target and with a Gaussian proposal that is very different from the target. 

In order to efficiently initialise your pMCMC, you can make use the results of the fit of the deterministic SEIT4L model of the previous session. Remember that you can load these results as follows:

```{r load-SEIT4L, collapse=TRUE}

# this should load 2 objects in your environment: mcmcSeit4lInfoPriorTheta1 and mcmcSeit4lInfoPriorTheta2. Each one is a list of 3 elements returned by mcmcMh
names(mcmcSeit4lInfoPriorTheta1)
```

In the previous session, you have initialised `mcmcMh` with a diagonal covariance matrix for the Gaussian proposal using the argument `proposalSd`. Actually, you can also pass a non-diagonal covariance matrix, accounting for correlations, by using the argument `covmat` (type `?mcmcMh` for more details).

You can now set the pMCMC by filling the empty bits below:

```{r set-pmcmc, eval=FALSE, tidy=FALSE}

# wrapper for posterior
my_posteriorSto <- function(theta){

    my_fitmodel <- # INSERT HERE
    my_initState <- # INSERT HERE

    return(dLogPosterior(fitmodel = my_fitmodel,
                         theta = theta,
                         initState = my_initState,
                         data = fluTdc1971,
                         margLogLike = my_particleFilter,
                         nParticles = nParticles))
    
}

# theta to initialise the pMCMC
initTheta <- # INSERT HERE

# covariance matrix for the Gaussian proposal
covmat <- # INSERT HERE

# lower and upper limits of each parameter (must be named vectors)
lower <- # INSERT HERE
upper <- # INSERT HERE

# number of iterations for the pMCMC
nIterations <- # INSERT HERE

# additional parameters for the adaptive pMCMC, see ?mcmcMh for more details
adaptSizeStart <- # INSERT HERE
adaptSizeCooling <- # INSERT HERE
adaptShapeStart <- # INSERT HERE
```

If you have trouble filling some of the empty bits, have a look at our [solution](pmcmc_example_solution.html#setting-the-pmcmc).

Then you should be able to run `mcmcMh`:

```{r run-pmcmc, eval=FALSE, tidy = FALSE}
# run the pMCMC
my_pMCMC <- mcmcMh(target = my_posteriorSto(400),
                   initTheta = initTheta,
                   covmat = covmat,
                   limits = list(lower = lower,upper = upper),
                   nIterations = nIterations,
                   adaptSizeStart = adaptSizeStart,
                   adaptSizeCooling = adaptSizeCooling,
                   adaptShapeStart = adaptShapeStart)
```

# Analyse a pMCMC with 50 particles

If you have run a pMCMC you should have noticed that it takes quite a lot of time as we need to run a particle filter at each iteration. Since the computation time scales linearly with the number of particles you might be tempted to reduce this number. Let's have a look at what we would get by running a pMCMC with 50 particles.

To save time, we have run 8 chains in parallel for you. Each chain was started from a different `initTheta` and ran for 3000 iterations. The values of `initTheta` were chosen close to the mean posterior estimates of the deterministic fit and their empirical covariance matrix was also used for the Gaussian proposal kernel. Each chain took 6 hours to complete on a scientific computing cluster and can be loaded as follows:

```{r load-trace, collapse=TRUE}
data(pmcmcSeit4lInfoPrior8)
# this should load a list with the same name, which contains the 8 chains.
names(pmcmcSeit4lInfoPrior8)
# each chain is a list of 3 elements returned by mcmcMh
names(pmcmcSeit4lInfoPrior8[["chain1"]])
# the trace contains 9 variables for 3000 iterations
dim(pmcmcSeit4lInfoPrior8[["chain1"]]$trace)
``` 
We can combine the traces of the 8 chains into a `mcmc.list` object as follows:

```{r combine-trace, collapse=TRUE}
library("coda")
trace <- mcmc.list(lapply(pmcmcSeit4lInfoPrior8, function(chain) {
  mcmc(chain$trace)
}))
head(trace, 1)
```
You can check that the chains were started from different `initState` and that we used informative priors.

__Take 15 min__ to analyse these chains and conclude on the choice of using 50 particles to save computation time.

You can compare your conclusions with [ours](pmcmc_example_solution.html#analyse-a-pmcmc-with-50-particles).

# Analyse a pMCMC with 400 particles

Let's increase the number of particles to 400, which is suggested by the calibration analysis. Again, to save time we have run 8 chains of 3000 iterations in parallel. Each chain took 40 hours to complete on a scientific computing cluster and can be loaded and stored in a `mcmc.list` object as follows:

```{r load-trace-400, collapse=TRUE}
# load
data(pmcmcSeit4lInfoPrior64)
# create
trace <- mcmc.list(lapply(pmcmcSeit4lInfoPrior8, function(chain) {
  mcmc(chain$trace)
}))
``` 

Re-do the same analysis as for 50 particles. What differences can you notice? In particular, try to compare the posterior distributions with 50 and 400 particles using the function `plotPosteriorDensity`.

You can also have a look at our [solution](pmcmc_example_solution.html#analyse-a-pmcmc-with-400-particles) if that helps.

# Stochastic vs deterministic fit

Compare the posterior of the deterministic and stochastic SEIT4L models. Which model provides the best fit? Can you explain why?

Check your answer [here](pmcmc_example_solution.html#stochastic-vs-deterministic-fit).

# Going further

## Filtered trajectories

* Actually, in addition to the log-likelihood, a particle filter can also return the filtered trajectories (i.e. all the trajectories that "survived" until the last observation time). You can update your filter so it keeps track and returns the filtered trajectories. Alternatively, there is a function in the package called `particleFilter` that will do it for you (see `?particleFilter` for documentation). Have a look at its code.
* If you run a particle filter with our function `particleFilter`, you can then plot the filtered trajectories using the function `plotSmc`.

## Optimization

You might have noted that the `for()` loop over particles could be parallelized, as particles can be propagated independently. You could take advantage of this to code a parallel loop and make your algorithm even faster. If you have never coded a parallel program in **R** you can also have a look at the code of `particleFilter`. Actually, all the test runs were performed on a scientific computing cluster with 12 core machines. So the computational time is expected to be multiplied by 12 without parallelization of the particle filter.

# References

- A [recent review](https://arxiv.org/abs/1703.02419) by Schön et al.
- A [comprehensive textbook](https://www.springer.com/gb/book/9780387951461) by Doucet et al.
- Another [review](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.117.1144) by Arulampalam et al.
data(mcmcTdcDeterLongRun)
